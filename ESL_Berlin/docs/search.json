[
  {
    "objectID": "computations.html",
    "href": "computations.html",
    "title": "Quarto Computations",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na"
  },
  {
    "objectID": "computations.html#numpy",
    "href": "computations.html#numpy",
    "title": "Quarto Computations",
    "section": "",
    "text": "import numpy as np\na = np.arange(15).reshape(3, 5)\na"
  },
  {
    "objectID": "computations.html#matplotlib",
    "href": "computations.html#matplotlib",
    "title": "Quarto Computations",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)"
  },
  {
    "objectID": "computations.html#plotly",
    "href": "computations.html#plotly",
    "title": "Quarto Computations",
    "section": "Plotly",
    "text": "Plotly\n\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "ESL Berlin Group Stage Bias Ratings",
    "section": "",
    "text": "import glob\nimport pandas as pd\nimport numpy as np\n\nimport difflib\n\nimport plotly.express as px\nimport plotly.io as pio\n\n\nresp_data = {}\ndiscord_names = []\nteam_names = []\n\nfor r_file in glob.glob(\"./data/*\"):\n    day = r_file[-5]\n    # print('day', r_file[-5], 'being parsed', end = '... ')\n    tmp = pd.read_csv(r_file)\n    \n    column_data = {}\n    \n    for c_n in tmp.columns:\n        if ' Vs ' in c_n or ' vs ' in c_n:\n            i = ''\n            if ' Vs ' in c_n:\n                i = c_n.split('Vs')\n            elif ' vs ' in c_n:\n                i = c_n.split('vs')\n\n            t_1 = i[0].strip()\n            t_2 = i[1][:-11].strip()\n            column_data[t_1 + ' v ' + t_2] = {'team_1': t_1, 'team_2': t_2}\n    \n    tmp.rename(columns={tmp.columns.values[1]: 'DISCORD_NAME_ID'}, inplace = True)\n    \n    tmp['DISCORD_NAME_ID'] = tmp.DISCORD_NAME_ID.fillna('????').astype(str)\n    tmp['DISCORD_NAME'] = tmp.DISCORD_NAME_ID.str.strip().apply(lambda x: x[:-5] if '#' in x else x).str.strip()\n    tmp['DISCORD_NUM'] = tmp.DISCORD_NAME_ID.str.strip().apply(lambda x: x[-4:] if '#' in x else '????').str.strip()\n    \n    # name columns and reorder columns\n    tmp.columns = np.concatenate((['SUBMIT_TS', 'DISCORD_NAME_ID'], list(column_data.keys()), ['DISCORD_NAME', 'DISCORD_NUM']))\n    tmp = tmp[list(np.concatenate((['SUBMIT_TS', 'DISCORD_NAME_ID', 'DISCORD_NAME', 'DISCORD_NUM'], list(column_data.keys()))))]\n    \n    for col in tmp.columns.drop(['SUBMIT_TS', 'DISCORD_NAME_ID', 'DISCORD_NAME', 'DISCORD_NUM']):\n        tmp[col] = tmp[col].str.strip('Victory').str.strip()\n    \n    # replacing the bad eggs, goddammit\n    tmp.replace({'Powdee': 'Powder',\n                 'waltz': 'Waltz',\n                 'cakes': 'Cakes'}, inplace = True)\n    \n    # replacing the misspelled teams, goddammit\n    tmp.replace({'Tunda Esports': 'Tundra Esports',\n                 '9 Panda': '9 Pandas',\n                 'Beastcaost': 'Beastcoast',\n                 'beastcoast': 'Beastcoast',\n                 'Gamin Gladiators': 'Gaimin Gladiators'}, inplace = True)\n    \n    column_data = pd.DataFrame(column_data).T.replace({'Tunda':' Tundra',\n                                                       '9 Panda': '9 Pandas',\n                                                       'Beastcaost': 'Beastcoast',\n                                                       'beastcoast': 'Beastcoast',\n                                                       'Gamin Gladiators': 'Gaimin Gladiators'})\n    \n    team_names = np.unique(np.append(np.array(team_names), np.unique(column_data.values)))\n    \n    discord_names = np.unique(np.append(np.array(discord_names), tmp.DISCORD_NAME.unique()))\n    \n    resp_data[day] = {'dataframe': tmp, 'col_data': column_data}\n    \n    # print('day', r_file[-5], '-', tmp.shape[0], 'responses parsed!')\n\n\nfor team in team_names:\n    if len(difflib.get_close_matches(team, team_names)) &gt; 1:\n        print(difflib.get_close_matches(team, team_names))\n\n['Talon Esports', 'Tundra Esports']\n['Team Liquid', 'Team Spirit']\n['Team SMG', 'Team Spirit']\n['Team Spirit', 'Team Liquid', 'Team SMG']\n['Tundra Esports', 'Talon Esports']\n\n\n\nfor name in discord_names:\n    if len(difflib.get_close_matches(name, discord_names)) &gt; 1:\n        print(difflib.get_close_matches(name, discord_names))\n\n\ndays = np.array(list(resp_data.keys()))\ndays.sort()\n\nresponder_biases = {}\n\nfor dn in discord_names:\n    responder_biases[dn] = {'DAYS_RESPONDED': 0, 'MATCHES_PREDICTED': 0}\n    for team in team_names:\n        responder_biases[dn][team] = 0.0\n\nfor day in days:\n    print('==============\\nday', day, 'reponses\\n==============')\n    day_info = resp_data[day]\n    day_frame = day_info['dataframe']\n    day_series = day_info['col_data']\n    day_frame = day_frame[day_frame.DISCORD_NAME != '????']\n    for dn in discord_names:\n        dn_data = day_frame[day_frame.DISCORD_NAME == dn]\n        \n        if dn_data.shape[0]:\n            responder_biases[dn]['DAYS_RESPONDED'] += 1\n            responder_biases[dn]['MATCHES_PREDICTED'] += day_series.shape[0]\n            for series in day_series.index:\n                response = day_frame[day_frame.DISCORD_NAME == dn][series].unique()[0]\n                \n                if response == 'Tie':\n                    responder_biases[dn][day_series.loc[series]['team_1']] += 0.5\n                    responder_biases[dn][day_series.loc[series]['team_2']] += 0.5\n                else:\n                    responder_biases[dn][response] += 1\n                # print('\\t', series, '\\t', day_frame[day_frame.DISCORD_NAME == dn][series].unique()[0])\n    # print()\n\n==============\nday 1 reponses\n==============\n==============\nday 2 reponses\n==============\n==============\nday 3 reponses\n==============\n==============\nday 4 reponses\n==============\n==============\nday 5 reponses\n==============\n\n\n\nr_bias_df = pd.DataFrame(responder_biases).T.fillna(0.0)\n\n\nr_bias_df.reset_index(names = 'DISCORD_NAME', inplace = True)\n\n\nr_bias_df['NA_BIAS'] = (r_bias_df['TSM'] + r_bias_df['Shopify Rebellion']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 2\nr_bias_df['SA_BIAS'] = (r_bias_df['Beastcoast'] + r_bias_df['Evil Geniuses']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 2\nr_bias_df['WEU_BIAS'] = (r_bias_df['Team Liquid'] + r_bias_df['Tundra Esports'] + r_bias_df['Gaimin Gladiators'] + r_bias_df['OG']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 4\nr_bias_df['EEU_BIAS'] = (r_bias_df['9 Pandas'] + r_bias_df['Team Spirit'] + r_bias_df['BetBoom Team']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 3\nr_bias_df['SEA_BIAS'] = (r_bias_df['Talon Esports'] + r_bias_df['Execration'] + r_bias_df['Team SMG']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 3\nr_bias_df['CN_BIAS'] = (r_bias_df['Xtreme Gaming'] + r_bias_df['PSG.LGD'] + r_bias_df['Invictus Gaming'] + r_bias_df['Team Aster']) / r_bias_df['MATCHES_PREDICTED'] * 18 / 4\n\n\nr_bias_df.describe()\n\n\n\n\n\n\n\n\nDAYS_RESPONDED\nMATCHES_PREDICTED\n9 Pandas\nBeastcoast\nBetBoom Team\nEvil Geniuses\nExecration\nGaimin Gladiators\nInvictus Gaming\nOG\n...\nTeam SMG\nTeam Spirit\nTundra Esports\nXtreme Gaming\nNA_BIAS\nSA_BIAS\nWEU_BIAS\nEEU_BIAS\nSEA_BIAS\nCN_BIAS\n\n\n\n\ncount\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n40.000000\n40.00000\n40.000000\n40.000000\n...\n40.000000\n40.000000\n40.00000\n40.000000\n39.000000\n39.000000\n39.000000\n39.000000\n39.000000\n39.000000\n\n\nmean\n4.050000\n59.400000\n2.800000\n3.312500\n3.125000\n3.300000\n1.012500\n5.42500\n1.850000\n4.087500\n...\n1.475000\n4.212500\n4.75000\n2.425000\n1.143258\n1.047991\n1.462540\n1.015339\n0.475694\n0.823561\n\n\nstd\n1.395046\n19.248443\n1.399634\n1.299346\n1.259579\n1.362501\n0.828092\n2.10783\n1.038984\n1.789884\n...\n0.823921\n1.811245\n1.94475\n1.525636\n0.226736\n0.291891\n0.176030\n0.198710\n0.143381\n0.156987\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.00000\n0.000000\n0.843750\n0.500000\n1.000000\n0.562500\n0.160714\n0.441964\n\n\n25%\n3.750000\n54.000000\n1.875000\n2.500000\n2.000000\n2.500000\n0.500000\n4.50000\n1.000000\n3.000000\n...\n0.875000\n3.000000\n3.75000\n1.500000\n0.992188\n0.879464\n1.351562\n0.838542\n0.375000\n0.777344\n\n\n50%\n5.000000\n72.000000\n3.000000\n3.000000\n3.000000\n3.500000\n0.750000\n6.00000\n1.750000\n4.500000\n...\n1.500000\n4.500000\n5.00000\n2.500000\n1.125000\n1.000000\n1.446429\n1.041667\n0.468750\n0.843750\n\n\n75%\n5.000000\n72.000000\n4.000000\n4.000000\n4.000000\n4.500000\n1.500000\n7.00000\n2.500000\n5.125000\n...\n2.000000\n5.500000\n6.50000\n3.000000\n1.200335\n1.218750\n1.600446\n1.145833\n0.583333\n0.925781\n\n\nmax\n5.000000\n72.000000\n5.500000\n6.500000\n6.000000\n5.500000\n3.500000\n8.00000\n4.000000\n7.500000\n...\n3.000000\n6.500000\n7.50000\n6.000000\n1.687500\n1.968750\n1.757812\n1.375000\n0.791667\n1.218750\n\n\n\n\n8 rows Ã— 26 columns\n\n\n\n\nr_bias_df.to_parquet('./results/esl_berlin_group_biases.gzip', compression = 'gzip')\nr_bias_df.to_csv('./results/esl_berlin_group_biases.csv')\n\n\nfig = px.scatter_3d(r_bias_df, x='NA_BIAS', y='WEU_BIAS', z='CN_BIAS', color='DISCORD_NAME', template = 'seaborn', height=1000)\nfig.show()\n\nfig.write_html(\"./results/esl_berlin_group_stage_bias.html\")"
  },
  {
    "objectID": "data_display.html",
    "href": "data_display.html",
    "title": "ESL Berlin Group Stage Biases in Capâ€™s Discord",
    "section": "",
    "text": "Oh look at that output there.\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\ndata = pd.read_parquet('./results/esl_berlin_group_biases.gzip')\n\nfig = px.scatter_3d(data, x='NA_BIAS', y='WEU_BIAS', z='CN_BIAS', color='DISCORD_NAME', template = 'seaborn', width=1000, height=1000)\nfig.show()"
  },
  {
    "objectID": "data_display.html#output",
    "href": "data_display.html#output",
    "title": "ESL Berlin Group Stage Biases in Capâ€™s Discord",
    "section": "",
    "text": "Oh look at that output there.\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\ndata = pd.read_parquet('./results/esl_berlin_group_biases.gzip')\n\nfig = px.scatter_3d(data, x='NA_BIAS', y='WEU_BIAS', z='CN_BIAS', color='DISCORD_NAME', template = 'seaborn', width=1000, height=1000)\nfig.show()"
  }
]